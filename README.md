# ğŸ§  NLP Tokenization from Scratch

This repository documents my exploration of how tokenization works under the hood â€” from simple whitespace splitting to Byte Pair Encoding (BPE), WordPiece, and SentencePiece.  
Itâ€™s part of my personal learning portfolio on NLP fundamentals and LLM pre-processing.

---

## ğŸš€ Goals

- Build and visualize tokenization algorithms from scratch.
- Understand how subword tokenization compresses vocabulary.
- Compare custom tokenizers with Hugging Face implementations.

---

## ğŸ“‚ Structure
| Folder | Description |
|---------|--------------|
| `notebooks/` | Jupyter notebooks for experimentation |
| `src/` | Python modules implementing tokenizers |
| `data/` | Small sample text corpora |

---

## âš™ï¸ Setup
```bash
git clone https://github.com/srkancharla/nlp-tokenization-from-scratch.git
cd nlp-tokenization-from-scratch
pip install -r requirements.txt
